{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"skipgram_imdb.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"cells":[{"cell_type":"code","metadata":{"id":"uyo_kKzIXR56","executionInfo":{"status":"ok","timestamp":1613406287946,"user_tz":-60,"elapsed":1083,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["import json\n","import numpy as np\n","import sys\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from scipy import stats\n","import pandas as pd \n","\n","from tqdm import tqdm\n","from itertools import islice\n","from nltk.corpus import stopwords\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","from scipy.spatial.distance import cosine"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"TjxThfJSYH3o"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6TJMxGfkXR5_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613376719203,"user_tz":-60,"elapsed":764,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"46b73841-8062-4a53-ebcd-53e3252903fb"},"source":["!jupyter nbextension enable --py widgetsnbextension"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Enabling notebook extension jupyter-js-widgets/extension...\n","      - Validating: \u001b[32mOK\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N_5o1zu9XR6A"},"source":["def get_windows(seq,n):\n","    '''\n","    returns a sliding window (of width n) over data from the iterable\n","    taken from: https://stackoverflow.com/questions/6822725/rolling-or-sliding-window-iterator/6822773#6822773\n","    '''\n","    it = iter(seq)\n","    result = tuple(islice(it, n))\n","    if len(result) == n:\n","        yield result\n","    for elem in it:\n","        result = result[1:] + (elem,)\n","        yield result\n","\n","\n","def sample_examples(docs,max_window_size,n_windows):\n","    '''generate target,context pairs and negative examples'''\n","    windows = []\n","    for i,doc in enumerate(docs):\n","        windows.append(list(get_windows(doc, 2*np.random.randint(1, max_window_size) + 1 )))\n","    windows = [elt for sublist in windows for elt in sublist] # flatten\n","    windows = list(np.random.choice(windows,size=n_windows)) # select a subset\n","    \n","    all_negs = list(np.random.choice(token_ints, size=n_negs*len(windows), p=neg_distr))\n","    return windows,all_negs\n","\n","def compute_dot_products(pos,negs,target):\n","    prods = Wc[pos+negs,] @ Wt[target,] # (n_pos+n_negs,d) X (d,) -> (n_pos+n_negs,)\n","    return prods\n","\n","def compute_loss(prodpos,prodnegs):\n","    '''prodpos and prodnegs are numpy vectors containing the dot products of the context word vectors with the target word vector'''\n","    term_pos, term_negs = np.log(1 + np.exp(-prodpos)), np.log(1 + np.exp(prodnegs))\n","    return np.sum(term_pos) + np.sum(term_negs)\n","\n","def compute_gradients(pos,negs,target,prodpos,prodnegs):\n","    factors_pos = 1/(np.exp(prodpos)+1)\n","    factors_negs = 1/(np.exp(-prodnegs)+1)\n","\n","    partial_pos = np.array([factors_pos[k,] * -Wt[target,] for k in range(len(factors_pos))])\n","    partial_negs = np.array([factors_negs[k,] * Wt[target,] for k in range(len(factors_negs))])\n","    \n","    term_pos = - Wc[pos,].T @ factors_pos\n","    term_negs = Wc[negs,].T @ factors_negs\n","    partial_target = np.sum(term_pos,axis=0) + np.sum(term_negs,axis=0)\n","    \n","    return partial_pos, partial_negs,partial_target\n","\n","# = = = = = = = = = = = = = = = = = = = = = \n","\n","max_window_size = 5 # extends on both sides of the target word\n","n_windows = int(1e6) # number of windows to sample at each epoch\n","n_negs = 5 # number of negative examples to sample for each positive\n","d = 64 # dimension of the embedding space\n","n_epochs = 15\n","lr_0 = 0.03\n","decay = 1e-6\n","\n","resume = True\n","train = True\n","\n","with open('/content/drive/MyDrive/nlp_centrale/imdb_files/doc_ints.txt', 'r') as file:\n","    docs = file.read().splitlines()\n","\n","docs = [[int(eltt) for eltt in elt.split()] for elt in docs]\n","\n","with open('/content/drive/MyDrive/nlp_centrale/imdb_files/vocab.json', 'r') as file:\n","    vocab = json.load(file)\n","\n","vocab_inv = {v:k for k,v in vocab.items()}\n","\n","with open('/content/drive/MyDrive/nlp_centrale/imdb_files/counts.json', 'r') as file:\n","    counts = json.load(file)\n","\n","token_ints = range(1,len(vocab)+1)\n","neg_distr = [counts[vocab_inv[elt]] for elt in token_ints]\n","neg_distr = np.sqrt(neg_distr)\n","neg_distr = neg_distr/sum(neg_distr) # normalize\n","\n","# ========== train model ==========\n","if train:\n","    \n","    total_its = int(1e6)*13\n","    if not resume :\n","        Wt = np.random.normal(size=(len(vocab)+1,d)) # + 1 is for the OOV token\n","        Wc = np.random.normal(size=(len(vocab)+1,d))\n","    else:\n","        Wt = np.load('/content/drive/MyDrive/nlp_centrale/imdb_files/input_vecs.npy')\n","        Wc = np.load('/content/drive/MyDrive/nlp_centrale/imdb_files/output_vecs.npy')\n","    \n","    for epoch in range(n_epochs):\n","        print(\"Epoch : %i/%i\"%(epoch+1, n_epochs))\n","        \n","        windows,all_negs = sample_examples(docs,max_window_size,n_windows)\n","        print('training examples sampled')\n","        \n","        np.random.shuffle(windows)\n","        \n","        total_loss = 0\n","        \n","        with tqdm(total=len(windows),unit_scale=True,postfix={'loss':0.0,'lr':lr_0},ncols=50) as pbar: #desc=\"Epoch : %i/%i\" % (epoch+1, n_epochs)\n","            for i,w in enumerate(windows):\n","                \n","                target = w[int(len(w)/2)] # elt at the center\n","                pos = list(w)\n","                del pos[int(len(w)/2)] # all elts but the center one\n","                \n","                negs = all_negs[n_negs*i:n_negs*i+n_negs]\n","                \n","                prods = compute_dot_products(pos,negs,target)\n","                prodpos = prods[0:len(pos),]\n","                prodnegs = prods[len(pos):(len(pos)+len(negs)),]\n","                \n","                partials_pos,partials_negs,partial_target = compute_gradients(pos,negs,target,prodpos,prodnegs)\n","                \n","                lr = lr_0 * 1/(1+decay*total_its)\n","                total_its += 1\n","                \n","                Wt[target,] -= lr * partial_target\n","                Wc[pos,] -= partials_pos * lr\n","                Wc[negs,] -= partials_negs * lr\n","                \n","                total_loss += compute_loss(prodpos,prodnegs)\n","                loss_printed = round(total_loss/(i+1), 4)\n","                lr_printed = round(lr, 4)\n","                pbar.set_postfix({\"loss\" : str(loss_printed), \"lr\" : str(lr_printed)})\n","                pbar.update(1)\n","        if epoch % 1 == 0:\n","            np.save('/content/drive/MyDrive/nlp_centrale/imdb_files/input_vecs',Wt,allow_pickle=False) # pickle disabled for portability reasons\n","            np.save('/content/drive/MyDrive/nlp_centrale/imdb_files/output_vecs',Wc,allow_pickle=False)\n","            print('word vectors saved to disk')\n","    \n","else:\n","    Wt = np.load('/content/drive/MyDrive/nlp_centrale/imdb_files/input_vecs.npy')\n","    Wc = np.load('/content/drive/MyDrive/nlp_centrale/imdb_files/output_vecs.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IXNxUmDrggyx","executionInfo":{"status":"ok","timestamp":1613406310689,"user_tz":-60,"elapsed":860,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["def my_cos_similarity(word1,word2):\n","    try:\n","      embed_1 = Wt[vocab[word1],].reshape(1,-1)\n","    except KeyError:\n","      embed_1 = Wt[0,].reshape(1,-1)\n","    try:\n","      embed_2 = Wt[vocab[word2],].reshape(1,-1)\n","    except KeyError:\n","      embed_2 = Wt[0,].reshape(1,-1)\n","    sim = cosine(embed_1, embed_2)\n","    return round(float(sim),4)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"PoNgUlSig9rI","executionInfo":{"status":"ok","timestamp":1613406312470,"user_tz":-60,"elapsed":808,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}}},"source":["def loadPairs(path):\n","    data = pd.read_csv(path, delimiter='\\t')\n","    pairs = zip(data['word1'], data['word2'], data['SimLex999'])\n","    return pairs"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9c02LzZgejI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613406313476,"user_tz":-60,"elapsed":577,"user":{"displayName":"Omar US","photoUrl":"","userId":"02556879631367095259"}},"outputId":"83174d90-9722-4000-aea5-d40a8ffebfb4"},"source":["pairs = loadPairs(\"/content/drive/MyDrive/nlp_centrale/SimLex-999.txt\")\n","our_similarities,original_similarities = [],[]\n","for a, b, original_similarity in pairs:\n","  our_similarities.append(my_cos_similarity(a, b)) \n","  original_similarities.append(original_similarity)\n","corr = stats.spearmanr(our_similarities,original_similarities).correlation\n","print('spearman correlation :',corr)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["spearman correlation : 0.03943479803014715\n"],"name":"stdout"}]}]}